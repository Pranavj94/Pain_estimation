{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmDvfFvxnGB",
        "outputId": "09f0662d-695e-4ca3-bef1-85df53c823e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tapnet'...\n",
            "remote: Enumerating objects: 541, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 541 (delta 126), reused 138 (delta 122), pack-reused 327\u001b[K\n",
            "Receiving objects: 100% (541/541), 1.23 MiB | 5.19 MiB/s, done.\n",
            "Resolving deltas: 100% (316/316), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Download Code {form-width: \"25%\"}\n",
        "!git clone https://github.com/deepmind/tapnet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sHBzdeQmX0r",
        "outputId": "f5d28b33-411e-4842-c537-101e3f23b72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhpufXp4753v",
        "outputId": "fb68a4bc-6893-479e-c27f-a89ab9421a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chex in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 2)) (0.4.14)\n",
            "Collecting jaxline (from -r tapnet/requirements_inference.txt (line 3))\n",
            "  Downloading jaxline-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 4)) (0.1.7)\n",
            "Collecting dm-haiku (from -r tapnet/requirements_inference.txt (line 5))\n",
            "  Downloading dm_haiku-0.0.10-py3-none-any.whl (360 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 6)) (0.1.8)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 7)) (4.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 8)) (3.7.1)\n",
            "Collecting mediapy (from -r tapnet/requirements_inference.txt (line 9))\n",
            "  Downloading mediapy-1.1.9-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 10)) (4.8.0.76)\n",
            "Collecting einshape (from -r tapnet/requirements_inference.txt (line 11))\n",
            "  Downloading einshape-1.0-py3-none-any.whl (21 kB)\n",
            "Collecting ipympl (from -r tapnet/requirements_inference.txt (line 12))\n",
            "  Downloading ipympl-0.9.3-py2.py3-none-any.whl (511 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r tapnet/requirements_inference.txt (line 13)) (4.66.1)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->-r tapnet/requirements_inference.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from chex->-r tapnet/requirements_inference.txt (line 1)) (0.4.14+cuda11.cudnn86)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from chex->-r tapnet/requirements_inference.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->-r tapnet/requirements_inference.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->-r tapnet/requirements_inference.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->-r tapnet/requirements_inference.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax->-r tapnet/requirements_inference.txt (line 2)) (1.10.1)\n",
            "Collecting ml_collections>=0.1 (from jaxline->-r tapnet/requirements_inference.txt (line 3))\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from jaxline->-r tapnet/requirements_inference.txt (line 3)) (1.14.1)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku->-r tapnet/requirements_inference.txt (line 5))\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->-r tapnet/requirements_inference.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r tapnet/requirements_inference.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mediapy->-r tapnet/requirements_inference.txt (line 9)) (7.34.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.10/dist-packages (from ipympl->-r tapnet/requirements_inference.txt (line 12)) (5.7.1)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipympl->-r tapnet/requirements_inference.txt (line 12)) (7.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9))\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (4.8.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (5.5.6)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (3.6.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (3.0.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections>=0.1->jaxline->-r tapnet/requirements_inference.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections>=0.1->jaxline->-r tapnet/requirements_inference.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections>=0.1->jaxline->-r tapnet/requirements_inference.txt (line 3)) (21.6.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy->-r tapnet/requirements_inference.txt (line 9)) (0.2.6)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (6.5.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (3.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (5.3.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.5.7)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (3.10.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (4.19.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.9.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.6.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl->-r tapnet/requirements_inference.txt (line 12)) (2.21)\n",
            "Building wheels for collected packages: jaxline, ml_collections\n",
            "  Building wheel for jaxline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaxline: filename=jaxline-0.0.5-py3-none-any.whl size=33132 sha256=b8d846797e3a941979062cd1b50441c2744a7a7cb02b39f55fbafdce16bba2f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/59/6b/d4d1e0bcba957ab4e84e80d1f8dcf528d5074a5561b6d13e00\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=00fd40ba3dd6a79c7fd192a3aeee0c0cb7f27b8e35c00c21b1deae770826b19b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built jaxline ml_collections\n",
            "Installing collected packages: ml_collections, jmp, jedi, einshape, dm-haiku, mediapy, jaxline, ipympl\n",
            "Successfully installed dm-haiku-0.0.10 einshape-1.0 ipympl-0.9.3 jaxline-0.0.5 jedi-0.19.0 jmp-0.0.4 mediapy-1.1.9 ml_collections-0.1.1\n"
          ]
        }
      ],
      "source": [
        "# @title Install Dependencies {form-width: \"25%\"}\n",
        "!pip install -r tapnet/requirements_inference.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaswJZMq9B3c",
        "outputId": "9a5841c0-3779-47ce-9c11-8d66c0b16c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-19 08:50:03--  https://storage.googleapis.com/dm-tapnet/causal_tapir_checkpoint.npy\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.12.128, 108.177.13.128, 74.125.26.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.12.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124408122 (119M) [application/octet-stream]\n",
            "Saving to: ‘tapnet/checkpoints/causal_tapir_checkpoint.npy’\n",
            "\n",
            "causal_tapir_checkp 100%[===================>] 118.64M   129MB/s    in 0.9s    \n",
            "\n",
            "2023-08-19 08:50:04 (129 MB/s) - ‘tapnet/checkpoints/causal_tapir_checkpoint.npy’ saved [124408122/124408122]\n",
            "\n",
            "causal_tapir_checkpoint.npy\n"
          ]
        }
      ],
      "source": [
        "# @title Download Model {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/checkpoints\n",
        "\n",
        "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/causal_tapir_checkpoint.npy\n",
        "\n",
        "%ls tapnet/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxlHY242m-6Q"
      },
      "outputs": [],
      "source": [
        "# @title Imports {form-width: \"25%\"}\n",
        "%matplotlib widget\n",
        "import functools\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tree\n",
        "\n",
        "from tapnet import tapir_model\n",
        "from tapnet.utils import transforms\n",
        "from tapnet.utils import viz_utils\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rfy2yobnHqw"
      },
      "outputs": [],
      "source": [
        "# @title Load Checkpoint {form-width: \"25%\"}\n",
        "\n",
        "checkpoint_path = 'tapnet/checkpoints/causal_tapir_checkpoint.npy'\n",
        "ckpt_state = np.load(checkpoint_path, allow_pickle=True).item()\n",
        "params, state = ckpt_state['params'], ckpt_state['state']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7wOMJoSQzq1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title Build Model {form-width: \"25%\"}\n",
        "\n",
        "# Internally, the tapir model has three stages of processing: computing\n",
        "# image features (get_feature_grids), extracting features for each query point\n",
        "# (get_query_features), and estimating trajectories given query features and\n",
        "# the feature grids where we want to track (estimate_trajectories).  For\n",
        "# tracking online, we need extract query features on the first frame only, and\n",
        "# then call estimate_trajectories on one frame at a time.\n",
        "\n",
        "def build_online_model_init(frames, query_points):\n",
        "  \"\"\"Initialize query features for the query points.\"\"\"\n",
        "  model = tapir_model.TAPIR(use_causal_conv=True, bilinear_interp_with_depthwise_conv=False)\n",
        "\n",
        "  feature_grids = model.get_feature_grids(frames, is_training=False)\n",
        "  query_features = model.get_query_features(\n",
        "      frames,\n",
        "      is_training=False,\n",
        "      query_points=query_points,\n",
        "      feature_grids=feature_grids,\n",
        "  )\n",
        "  return query_features\n",
        "\n",
        "\n",
        "def build_online_model_predict(frames, query_features, causal_context):\n",
        "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
        "  model = tapir_model.TAPIR(use_causal_conv=True, bilinear_interp_with_depthwise_conv=False)\n",
        "  feature_grids = model.get_feature_grids(frames, is_training=False)\n",
        "  trajectories = model.estimate_trajectories(\n",
        "      frames.shape[-3:-1],\n",
        "      is_training=False,\n",
        "      feature_grids=feature_grids,\n",
        "      query_features=query_features,\n",
        "      query_points_in_video=None,\n",
        "      query_chunk_size=64,\n",
        "      causal_context=causal_context,\n",
        "      get_causal_context=True,\n",
        "  )\n",
        "  causal_context = trajectories['causal_context']\n",
        "  del trajectories['causal_context']\n",
        "  return {k: v[-1] for k, v in trajectories.items()}, causal_context\n",
        "\n",
        "\n",
        "online_init = hk.transform_with_state(build_online_model_init)\n",
        "online_init_apply = jax.jit(online_init.apply)\n",
        "\n",
        "online_predict = hk.transform_with_state(build_online_model_predict)\n",
        "online_predict_apply = jax.jit(online_predict.apply)\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "online_init_apply = functools.partial(\n",
        "    online_init_apply, params=params, state=state, rng=rng\n",
        ")\n",
        "online_predict_apply = functools.partial(\n",
        "    online_predict_apply, params=params, state=state, rng=rng\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogRTRVgfSq0W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title Utility Functions {form-width: \"25%\"}\n",
        "\n",
        "def preprocess_frames(frames):\n",
        "  \"\"\"Preprocess frames to model inputs.\n",
        "\n",
        "  Args:\n",
        "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
        "\n",
        "  Returns:\n",
        "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
        "  \"\"\"\n",
        "  frames = frames.astype(np.float32)\n",
        "  frames = frames / 255 * 2 - 1\n",
        "  return frames\n",
        "\n",
        "\n",
        "def postprocess_occlusions(occlusions, expected_dist):\n",
        "  \"\"\"Postprocess occlusions to boolean visible flag.\n",
        "\n",
        "  Args:\n",
        "    occlusions: [num_points, num_frames], [-inf, inf], np.float32\n",
        "\n",
        "  Returns:\n",
        "    visibles: [num_points, num_frames], bool\n",
        "  \"\"\"\n",
        "  pred_occ = jax.nn.sigmoid(occlusions)\n",
        "  pred_occ = 1 - (1 - pred_occ) * (1 - jax.nn.sigmoid(expected_dist))\n",
        "  visibles = pred_occ < 0.5  # threshold\n",
        "  return visibles\n",
        "\n",
        "\n",
        "def sample_random_points(frame_max_idx, height, width, num_points):\n",
        "  \"\"\"Sample random points with (time, height, width) order.\"\"\"\n",
        "  y = np.random.randint(0, height, (num_points, 1))\n",
        "  x = np.random.randint(0, width, (num_points, 1))\n",
        "  t = np.random.randint(0, frame_max_idx + 1, (num_points, 1))\n",
        "  points = np.concatenate((t, y, x), axis=-1).astype(np.int32)  # [num_points, 3]\n",
        "  return points\n",
        "\n",
        "\n",
        "def construct_initial_causal_state(num_points, num_resolutions):\n",
        "  value_shapes = {\n",
        "      \"tapir/~/pips_mlp_mixer/block_1_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_1_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_2_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_2_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_3_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_3_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_4_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_4_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_5_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_5_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_6_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_6_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_7_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_7_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_8_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_8_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_9_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_9_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_10_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_10_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_11_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_11_causal_2\": (1, num_points, 2, 2048),\n",
        "      \"tapir/~/pips_mlp_mixer/block_causal_1\": (1, num_points, 2, 512),\n",
        "      \"tapir/~/pips_mlp_mixer/block_causal_2\": (1, num_points, 2, 2048),\n",
        "  }\n",
        "  fake_ret = {\n",
        "      k: jnp.zeros(v, dtype=jnp.float32) for k, v in value_shapes.items()\n",
        "  }\n",
        "  return [fake_ret] * num_resolutions * 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_wvP2ook6UrA",
        "outputId": "b4b2910c-21bf-4966-aa82-bd68234b7b8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Person         video                     frame  Label\n",
              "0  042-ll042  ll042t1aaaff  ll042t1aaaff001_facs.txt      0\n",
              "1  042-ll042  ll042t1aaaff  ll042t1aaaff002_facs.txt      0\n",
              "2  042-ll042  ll042t1aaaff  ll042t1aaaff003_facs.txt      0\n",
              "3  042-ll042  ll042t1aaaff  ll042t1aaaff004_facs.txt      0\n",
              "4  042-ll042  ll042t1aaaff  ll042t1aaaff005_facs.txt      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8f9cc1e-d651-433b-9aac-208e6fe4300f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Person</th>\n",
              "      <th>video</th>\n",
              "      <th>frame</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>042-ll042</td>\n",
              "      <td>ll042t1aaaff</td>\n",
              "      <td>ll042t1aaaff001_facs.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>042-ll042</td>\n",
              "      <td>ll042t1aaaff</td>\n",
              "      <td>ll042t1aaaff002_facs.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>042-ll042</td>\n",
              "      <td>ll042t1aaaff</td>\n",
              "      <td>ll042t1aaaff003_facs.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>042-ll042</td>\n",
              "      <td>ll042t1aaaff</td>\n",
              "      <td>ll042t1aaaff004_facs.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>042-ll042</td>\n",
              "      <td>ll042t1aaaff</td>\n",
              "      <td>ll042t1aaaff005_facs.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8f9cc1e-d651-433b-9aac-208e6fe4300f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b8f9cc1e-d651-433b-9aac-208e6fe4300f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b8f9cc1e-d651-433b-9aac-208e6fe4300f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e6f5d2f-fbfc-4871-8fcd-2f1a3ca5eccc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e6f5d2f-fbfc-4871-8fcd-2f1a3ca5eccc')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e6f5d2f-fbfc-4871-8fcd-2f1a3ca5eccc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "frame_labels = pd.read_csv('/content/drive/MyDrive/Dissertation_2/filtered_pain_labels.csv')\n",
        "frame_labels.head()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG8woxuTtTQx"
      },
      "outputs": [],
      "source": [
        "#  Load image from folder\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "label_dict=dict()\n",
        "\n",
        "\n",
        "def load_video(folder_path):\n",
        "\n",
        "\n",
        "  # Get a list of all PNG files in the folder\n",
        "  png_files = [file for file in os.listdir(folder_path) if file.endswith(\".png\")]\n",
        "  png_files.sort()\n",
        "  # Create an empty list to store the loaded images\n",
        "  loaded_images = []\n",
        "  label_list=list()\n",
        "\n",
        "  # Iterate over each PNG file and load it\n",
        "  for file_name in png_files:\n",
        "\n",
        "\n",
        "      # Construct the full file path\n",
        "      file_path = os.path.join(folder_path, file_name)\n",
        "      splits = file_path.split('/')\n",
        "      #print(splits)\n",
        "\n",
        "      condition1 = frame_labels['Person']==str(splits[-3])\n",
        "      condition2 = frame_labels['video']==str(splits[-2])\n",
        "      condition3 = frame_labels['frame']==str(splits[-1]).split('.')[0]+'_facs.txt'\n",
        "\n",
        "      label = frame_labels.loc[condition1 & condition2 & condition3,'Label'].values[0]\n",
        "      label_list.append(label)\n",
        "\n",
        "      # Open the image using PIL\n",
        "      image = Image.open(file_path)\n",
        "      new_size = (320, 240)\n",
        "      image = image.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "      # Convert the image to RGB mode (in case it has an alpha channel)\n",
        "      image = image.convert(\"RGB\")\n",
        "\n",
        "      # Append the image to the list\n",
        "      loaded_images.append(image)\n",
        "\n",
        "  label_dict[splits[-2]]=label_list\n",
        "  # Determine the dimensions of the images\n",
        "  num_images = len(loaded_images)\n",
        "  #height, width = loaded_images[0].size\n",
        "  height, width = 240, 320\n",
        "\n",
        "  # Create a NumPy array of size (number of images, height, width, 3)\n",
        "  image_array = np.zeros((num_images, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "  # Fill the array with the image data\n",
        "  for i, image in enumerate(loaded_images):\n",
        "      image_array[i] = np.array(image)\n",
        "\n",
        "  # Print the shape of the image array\n",
        "  #print(\"Shape of the image array:\", image_array.shape)\n",
        "\n",
        "  video = image_array\n",
        "  return(video)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_arw0Fwwk3so"
      },
      "outputs": [],
      "source": [
        "def detect_landmarks(image_path):\n",
        "  # STEP 3: Load the input image.\n",
        "  image = mp.Image.create_from_file(image_path)\n",
        "\n",
        "  # STEP 4: Detect face landmarks from the input image.\n",
        "  detection_result = detector.detect(image)\n",
        "\n",
        "  landmark_targets = list([130,25,243,244,245,6,196,197,193,168,\n",
        "                     417, 419, 413,414,398,465,464,463,362,255,359,\n",
        "                     8,9,55,107,336,285,65,52,53,46,124,156,226,35,143,31,295,282,283,276,265,446,261,353,372,340,368,264,\n",
        "                      57,185,40,39,37,0,267,269,270,409,287,375,321,405,314,17,84,181,91,146,61,76,62,78,308,293,306,291,\n",
        "                      122,188,114,47,100,101,50,123,137,351,412,343,277,329,330,280,352,366,\n",
        "                      129,203,206,216,212,210,169,358,423,426,436,432,430,394,\n",
        "                         246,161,160,159,158,157,173,133,154,155,153,145,144,163,7,\n",
        "                         398,384,385,386,387,388,466,263,249,390,373,374,380,381,382,362])\n",
        "\n",
        "  landmark_list=list()\n",
        "  projection_image = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "  face_landmarks_list = detection_result.face_landmarks\n",
        "  for i,landmark in enumerate(face_landmarks_list[0]):\n",
        "    if i in landmark_targets:\n",
        "      x = landmark.x\n",
        "      y = landmark.y\n",
        "\n",
        "      shape = projection_image.shape\n",
        "      relative_x = int(x * shape[1])\n",
        "      relative_y = int(y * shape[0])\n",
        "\n",
        "      landmark_list.append([0,relative_y,relative_x])\n",
        "\n",
        "  lardmark_array = np.array(landmark_list)\n",
        "\n",
        "  return (lardmark_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for sequence in video sequence folders:\n",
        "  Video = Load video(sequences)\n",
        "  First_image = Identify first image in the sequence\n",
        "  First_image_Landmark = Mediapipe(First image)\n",
        "  Tracked_landmarks = TAPIR (Video, First_image_Landmark)\n",
        "Rolled landmarks = Rolling window logic (Merged landmarks)\n",
        "Rolled labels = Rolling window logic (Labels)\n",
        "Face extracts = Mediapipe (Frames)\n",
        "Frame pain estimate model (Rolled landmarks,Face extracts,Rolled labels)\n",
        "Video pain estimate model (Tracked_landmarks,Face extracts,labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "wHeE5CO4OkUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JolKCKujg_Ae",
        "outputId": "5de32bc4-d591-4b40-8e67-78d43e398584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBza5ZgEhAmd"
      },
      "outputs": [],
      "source": [
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xbbQMpKowLJ"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "# STEP 2: Create an FaceLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
        "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
        "                                       output_face_blendshapes=True,\n",
        "                                       output_facial_transformation_matrixes=True,\n",
        "                                       num_faces=1)\n",
        "detector = vision.FaceLandmarker.create_from_options(options)\n",
        "\n",
        "def track_points(video,landmark_array):\n",
        "    # @title Progressively Predict Sparse Point Tracks {form-width: \"25%\"}\n",
        "\n",
        "  resize_height = 240  # @param {type: \"integer\"}\n",
        "  resize_width = 320  # @param {type: \"integer\"}\n",
        "\n",
        "  #height, width = video.shape[1:3]\n",
        "  height, width = 240, 320\n",
        "  frames = media.resize_video(video, (resize_height, resize_width))\n",
        "  query_points = landmark_array\n",
        "\n",
        "  query_features, _ = online_init_apply(frames=preprocess_frames(frames[None, None, 0]), query_points=query_points[None])\n",
        "  causal_state = construct_initial_causal_state(query_points.shape[0], len(query_features.resolutions) - 1)\n",
        "\n",
        "  # Predict point tracks frame by frame\n",
        "  predictions = []\n",
        "  print('Frame size',frames.shape[0])\n",
        "  for i in range(frames.shape[0]):\n",
        "    (prediction, causal_state), _ = online_predict_apply(\n",
        "        frames=preprocess_frames(frames[None, None, i]),\n",
        "        query_features=query_features,\n",
        "        causal_context=causal_state,\n",
        "    )\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  tracks = np.concatenate([x['tracks'][0] for x in predictions], axis=1)\n",
        "\n",
        "  # Visualize sparse point tracks\n",
        "  tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (width, height))\n",
        "\n",
        "  occlusions = np.concatenate([x['occlusion'][0] for x in predictions], axis=1)\n",
        "  expected_dist = np.concatenate([x['expected_dist'][0] for x in predictions], axis=1)\n",
        "\n",
        "  visibles = postprocess_occlusions(occlusions, expected_dist)\n",
        "\n",
        "  # # Visualize sparse point tracks\n",
        "  # tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (width, height))\n",
        "  video_viz = viz_utils.paint_point_track(video, tracks, visibles)\n",
        "  media.show_video(video_viz, fps=10)\n",
        "  return(tracks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "jx7Pq7MSqC20",
        "outputId": "922fbe2f-2298-4107-c281-fd907f5d27f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107-hs107\n",
            "hs107t2aaaff\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-37f223bd808d>:39: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize(new_size, Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First_file_path /content/drive/MyDrive/Dissertation_2/Images/Images/107-hs107/hs107t2aaaff/hs107t2aaaff001.png\n",
            "Frame size 412\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-a24812607977>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mtracked_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_track\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0mtrajectory_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracked_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-a24812607977>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(track_folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'First_file_path'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfirst_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_loaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmark_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-c3ab68dff16e>\u001b[0m in \u001b[0;36mtrack_points\u001b[0;34m(video, landmark_array)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frame size'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     (prediction, causal_state), _ = online_predict_apply(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mquery_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def main(track_folder):\n",
        "  video_loaded = load_video(track_folder)\n",
        "\n",
        "  folder_path = Path(track_folder)\n",
        "  files = list(folder_path.iterdir())\n",
        "  dir_files=list()\n",
        "  for i in files:\n",
        "    if str(i). split('.')[1]=='png':\n",
        "      dir_files.append(str(i))\n",
        "  dir_files.sort()\n",
        "  first_file_path = dir_files[0]\n",
        "\n",
        "  landmark_points = detect_landmarks(first_file_path)\n",
        "  print('First_file_path',first_file_path)\n",
        "\n",
        "  trajectory = track_points(video_loaded, landmark_points)\n",
        "\n",
        "  return(trajectory)\n",
        "\n",
        "trajectory_main= dict()\n",
        "folder_track = '/content/drive/MyDrive/Dissertation_2/Images/Images/'\n",
        "for files in os.listdir(folder_track):\n",
        "  print(files)\n",
        "  for folders in os.listdir(folder_track+files):\n",
        "    if (folders in list(set(frame_labels['video']))) and (folders not in list(trajectory_main.keys())):\n",
        "      print(folders)\n",
        "      tracked_points = main(folder_track+files + '/' + folders)\n",
        "      trajectory_main[folders] = np.array(tracked_points)\n",
        "      #break\n",
        "      #trajectory_main.append(np.array(tracked_points))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64P7s81quSRW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "# Specify the file path to save the dictionary\n",
        "file_path = '/content/drive/MyDrive/Dissertation_2/trajectory.pickle'\n",
        "\n",
        "# Save the dictionary using pickle\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(trajectory_main, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVptpJ5TH_Jf"
      },
      "outputs": [],
      "source": [
        "# Specify the file path to save the dictionary\n",
        "file_path = '/content/drive/MyDrive/Dissertation_2/labels.pickle'\n",
        "\n",
        "# Save the dictionary using pickle\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(label_dict, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jk4gRoluFg_"
      },
      "outputs": [],
      "source": [
        " # trajectory_array=np.array(trajectory_main)\n",
        "# trajectory_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0XuvZFVdOvD"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# import os\n",
        "\n",
        "# input_folder = '/content/drive/MyDrive/Dissertation_2/Images/Images/123-jh123/jh123t1aeunaff'  # Specify the path to the folder containing the images\n",
        "# output_folder = '/content/drive/MyDrive/Dissertation_2/Images/Images/123-jh123/jh123t1aeunaff'  # Specify the path to the folder where resized images will be saved\n",
        "\n",
        "# # Ensure the output folder exists\n",
        "# if not os.path.exists(output_folder):\n",
        "#     os.makedirs(output_folder)\n",
        "\n",
        "# main_folder = '/content/drive/MyDrive/Dissertation_2/Images/Images'\n",
        "\n",
        "# for folders in os.listdir(main_folder):\n",
        "\n",
        "\n",
        "\n",
        "# # Loop through all files in the input folder\n",
        "# for folders in\n",
        "# for filename in os.listdir(input_folder):\n",
        "#     if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "#         image_path = os.path.join(input_folder, filename)\n",
        "#         output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "#         # Open the image\n",
        "#         img = Image.open(image_path)\n",
        "\n",
        "#         # Resize the image\n",
        "#         # You can adjust the new size as needed\n",
        "#         new_size = (320, 240)\n",
        "#         resized_img = img.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "#         # Save the resized image\n",
        "#         resized_img.save(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEmaeMu7gE5o"
      },
      "outputs": [],
      "source": [
        "# main_folder = '/content/drive/MyDrive/Dissertation_2/Images/Images/'\n",
        "\n",
        "# for folders in os.listdir(main_folder):\n",
        "#   for folder_2 in os.listdir(main_folder+folders):\n",
        "#     for filename in os.listdir(main_folder+folders+'/'+folder_2):\n",
        "#       if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "#         image_path = os.path.join(main_folder,folders,folder_2, filename)\n",
        "#         output_path = os.path.join(main_folder,folders,folder_2, filename)\n",
        "#         # Open the image\n",
        "#         img = Image.open(image_path)\n",
        "\n",
        "#         # Resize the image\n",
        "#         # You can adjust the new size as needed\n",
        "#         new_size = (320, 240)\n",
        "#         resized_img = img.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "#         # Save the resized image\n",
        "#         resized_img.save(output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBUZpSwpdzl2"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# import os\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# img = Image.open('/content/drive/MyDrive/Dissertation_2/Images/Images/123-jh123/jh123t1aeunaff/jh123t1aeunaff001.png')\n",
        "# print(np.array(img).shape)\n",
        "# new_size = (320, 240)\n",
        "# resized_img = img.resize(new_size, Image.ANTIALIAS)\n",
        "# np.array(resized_img).shape\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}